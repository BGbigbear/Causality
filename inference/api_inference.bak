import json
import logging
import sys
import time
import os

from openai import OpenAI

from config.path import *
from config.prompt_list import *
from util.rag import create_vector_database, load_retriever
from util.rouge import top_similar_text

client = OpenAI(
    api_key="4GT75Atw94Q4j044iBAT1AK85NreqXJU",
    base_url="https://api.deepinfra.com/v1/openai",
)

logging.disable(logging.WARNING)
logging.basicConfig(
    level=logging.WARNING,  # 设置日志级别
    # format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',  # 日志格式
    # datefmt='%Y-%m-%d %H:%M:%S',  # 日期格式
    # filename='app.log',  # 日志输出到文件
    # filemode='w'  # 写模式，'a'为追加模式
)


def progress_bar(i, n, length=50, extra_info=""):
    percent = i / n
    # Calculate the number of '=' to display in the progress bar
    filled_length = int(length * percent)
    # Create the progress bar string
    bar = '=' * filled_length + '-' * (length - filled_length)
    # Print the progress bar with the percentage
    sys.stdout.write(f'\r[{bar}] {percent:.2%} {extra_info}')
    sys.stdout.flush()


def convert_seconds(seconds):
    if seconds < 60:
        return f"{seconds:.0f} s"
    elif seconds < 3600:
        minutes = seconds // 60
        remaining_seconds = seconds % 60
        return f"{minutes:.0f} m {remaining_seconds:.0f} s"
    else:
        hours = seconds // 3600
        minutes = (seconds % 3600) // 60
        remaining_seconds = seconds % 60
        return f"{hours:.0f} h {minutes:.0f} m {remaining_seconds:.0f} s"


def chat_completion(messages, model="google/gemma-2-27b-it"):
    completion = client.chat.completions.create(
        model=model,
        messages=messages
    )
    return completion


def chat(text, few_shot=None, prompts=causality_prompts_v0):
    # q1 = f"文本：{text}" + f"\n参考：\n{few_shot}" if few_shot else f"文本：{text}"
    # q1 += f"\n({prompts[1]})"
    #
    # messages = [
    #     {"role": "system", "content": prompts[0]},
    #     {"role": "user", "content": q1}
    # ]

    messages = [
        {"role": "user", "content": f"{few_shot}\n\n{prompts}\n\n\"text\": {text}"}
    ]
    completion = chat_completion(messages)
    messages.append({"role": "assistant", "content": str(completion.choices[0].message.content)})

    for i in range(1, len(prompts)):
        if i > 1:
            messages.append({"role": "user", "content": prompts[i]})
        completion = chat_completion(messages)
        messages.append({"role": "assistant", "content": str(completion.choices[0].message.content)})

    return messages


def check_json_structure(json_data):
    required_keys = {
        "actor", "class", "action", "time", "location", "object"
    }
    event_classes = ["经济事件", "科技发展", "军事行动", "安全事件", "航空航天活动", "装备与军备", "社会事件", "外交活动", "政治事件"]

    if "causality_list" not in json_data:
        print(f"Document {json_data.get('document_id', 'Unknown')} is missing 'causality_list'.")
        return False

    for idx, causality in enumerate(json_data["causality_list"]):
        for role in ["cause", "effect"]:
            event = causality.get(role, {})

            if event['class'] not in event_classes:
                print(f"Document {json_data.get('document_id', 'Unknown')}, Event {idx}, '{role}' type error: {event['class']}.")
                return False

            missing_keys = required_keys - event.keys()
            extra_keys = event.keys() - required_keys

            if missing_keys:
                print(f"Document {json_data.get('document_id', 'Unknown')}, Event {idx}, '{role}' is missing keys: {missing_keys}.")
                return False
            if extra_keys:
                print(f"Document {json_data.get('document_id', 'Unknown')}, Event {idx}, '{role}' has extra keys: {extra_keys}.")
                return False

    return True


def generate(start_point=0, end_point=0, rouge=False, rag=False):
    with open(test_file, 'r', encoding='utf-8') as f_test:
        test_data = json.load(f_test)

    # using RAG
    causality_data, retriever = None, None
    if rag or rouge:
        with open(causality_file, 'r', encoding='utf-8') as f_causality:
            causality_data = json.load(f_causality)
        if rag:
            retriever = load_retriever(causality_data, db_path)

    start_time = time.time()
    n = len(test_data) if start_point == end_point == 0 \
        else (len(test_data) - start_point if end_point == 0 else end_point - start_point)  # total task number
    msg_data, result_data = [], []

    if start_point != 0 and os.path.exists(analysis_file) and os.path.exists(pred_file):
        with (
            open(analysis_file, 'r', encoding='utf-8') as f_analysis,
            open(pred_file, 'r', encoding='utf-8') as f_pred
        ):
            msg_data, result_data = json.load(f_analysis), json.load(f_pred)

    progress_bar(0, n)

    try:
        for i, doc in enumerate(test_data):
            if start_point != 0 and i < start_point:
                # progress_bar(i - start_point + 1, n)
                continue
            if end_point != 0 and i == end_point:
                break

            few_shot = ""
            if rouge:
                shots = {"Event_extraction_examples": top_similar_text(doc['text'], causality_data)}
                few_shot += json.dumps(shots, ensure_ascii=False, indent=2)
            elif rag:
                vector_search_results = retriever.invoke(f"{doc['text']}")
                shots = []
                for search_result in vector_search_results:
                    content = search_result.page_content.split('_', maxsplit=1)
                    shot = {"text": causality_data[int(content[0])]['text'],
                            "causality_list": causality_data[int(content[0])]['causality_list']}
                    shots.append(shot)
                few_shot += json.dumps(shots, ensure_ascii=False, indent=2)

            msgs = chat(doc['text'], few_shot, causality_prompts_3shots)  # generate
            content = msgs[-1]['content']

            max_retries, retries, circle_flag = 5, 0, False
            while retries < max_retries:
                if circle_flag:
                    # completion = chat_completion(msgs[:-1])
                    # content = str(completion.choices[0].message.content)
                    content = chat(doc['text'], few_shot, causality_prompts_3shots)[-1]['content']
                try:
                    result = json.loads(content[content.find('{'): content.rfind(']') + 1] + "\n}")
                    if not check_json_structure(result):
                        logging.error(f"JSON parsing failed on attempt {retries + 1} of document_{doc['document_id']}: "
                                      f"Json structure error.")
                        circle_flag = True
                        retries += 1
                        continue
                    msg_data.append({"document_id": doc['document_id'], "text": doc['text'], "analysis": msgs})
                    result_data.append({"document_id": doc['document_id'], "text": doc['text'], **result})
                    break
                except ValueError as e:
                    logging.error(f"JSON parsing failed on attempt {retries + 1} of document_{doc['document_id']}: {e}")
                    circle_flag = True
                    retries += 1

            if retries == max_retries:
                logging.error(f"Failed to process document {doc['document_id']} after {max_retries} attempts.")

            extra_info = f"Task {i - start_point + 1}/{n} | Elapsed Time: {convert_seconds(time.time() - start_time)}"
            progress_bar(i - start_point + 1, n, extra_info=extra_info)
    finally:
        with (
            open(analysis_file, 'w', encoding='utf-8') as f_analysis,
            open(pred_file, 'w', encoding='utf-8') as f_pred
        ):
            json.dump(msg_data, f_analysis, ensure_ascii=False, indent=4)
            json.dump(result_data, f_pred, ensure_ascii=False, indent=4)


if __name__ == '__main__':
    pass
